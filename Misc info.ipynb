{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SO etc\n",
    "1. [SO: tuninq pong/dqn](https://ai.stackexchange.com/questions/10306/each-training-run-for-ddqn-agent-takes-2-days-and-still-ends-up-with-13-avg-sc/10481#10481)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English articles:\n",
    "\n",
    "1. [Policy Gradient Algorithms](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)\n",
    "1. [Actor-Critic Methods: A3C and A2C](https://danieltakeshi.github.io/2018/06/28/a2c-a3c/)\n",
    "1. [Notes on the Generalized Advantage Estimation Paper](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/)\n",
    "1. [Speeding up DQN on PyTorch: how to solve Pong in 30 minutes](https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55)\n",
    "1. [Intro to Advantage-Actor-Critic (A2C)](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752)\n",
    "1. [Intuitive RL: Intro to Advantage-Actor-Critic (A2C)](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752)\n",
    "1. [RL — DQN Deep Q-network](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4)\n",
    "1. [Implementing the A3C Algorithm to train an Agent to play Breakout! (+ example of LSTM in RL)](https://medium.com/@shagunm1210/implementing-the-a3c-algorithm-to-train-an-agent-to-play-breakout-c0b5ce3b3405)\n",
    "1. [Beat Atari with Deep Reinforcement Learning! (Part 1: DQN)](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)\n",
    "1. [An introduction to Policy Gradients with Cartpole and Doom](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)\n",
    "1. [Policy Gradients in a Nutshell](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)\n",
    "1. [Let’s make a DQN: Double Learning and Prioritized Experience Replay](https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/)\n",
    "1. [Advanced DQNs: Playing Pac-man with Deep Reinforcement Learning](https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Books\n",
    "1. [Reinforcement Learning: An Introduction](https://cs.wmich.edu/~trenary/files/cs5300/RLBook/the-book.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По-русски:\n",
    "\n",
    "1. [перевод старой статьи на хабре](https://habr.com/ru/post/439674/)\n",
    "1. [Интуитивный RL (Reinforcement Learning): введение в Advantage-Actor-Critic (A2C)](https://habr.com/ru/post/442522/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc articles, english\n",
    "\n",
    "1. [Sum Tree](https://www.fcodelabs.com/2019/03/18/Sum-Tree-Introduction/)\n",
    "1. [Attention in NLP](https://medium.com/@joealato/attention-in-nlp-734c6fa9d983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### github:\n",
    "\n",
    "1. [неплохие репы](https://github.com/higgsfield)\n",
    "1. [DQN CartPole на keras](https://github.com/keon/deep-q-learning)\n",
    "1. [так себе PG (Pong) на keras](https://github.com/keon/policy-gradient)\n",
    "1. [разное, немного про RL](https://github.com/lcswillems)\n",
    "1. [Quantile Regression DQN](https://github.com/senya-ashukha/quantile-regression-dqn-pytorch)\n",
    "1. [Exploration by Random Network Distillation](https://github.com/openai/random-network-distillation)\n",
    "1. [RL Tutorials, check plot() in utils](https://github.com/qfettes/DeepRL-Tutorials)\n",
    "1. [sum tree](https://github.com/chuyangliu/Snake/blob/master/snake/util/sumtree.py)\n",
    "1. [pretty clean implementations of DQN/AC](https://github.com/germain-hug/Deep-RL-Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### misc repos\n",
    "\n",
    "1. [net architecture -> latex](https://github.com/HarisIqbal88/PlotNeuralNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pdfs:\n",
    "\n",
    "1. [Hierarchical Deep Reinforcement Learning](http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf)\n",
    "1. [Exploration by Random Network Distillation](https://arxiv.org/pdf/1810.12894.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[markdown cheatsheet](https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
