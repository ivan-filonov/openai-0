{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### based on https://github.com/higgsfield/RL-Adventure and https://medium.com/swlh/introduction-to-reinforcement-learning-coding-sarsa-part-4-2d64d6e37617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "resource.setrlimit(resource.RLIMIT_RSS, (32 << 30, 32 << 30))\n",
    "#resource.getrusage(resource.RUSAGE_SELF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch as t\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not belive how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = np.concatenate(self._frames, axis=0)\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Change image shape to CWH\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n",
    "    \n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = collections.deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ResizeObservation, self).__init__(env)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0] // 2, shp[1] // 2, shp[2]))\n",
    "        self.resize_to = (shp[1] // 2, shp[0] // 2)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return cv2.resize(observation, self.resize_to, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "SUFFIX = 'NoFrameskip-v4'\n",
    "env = gym.make('Pong' + SUFFIX)\n",
    "env = ResizeObservation(env)\n",
    "env = ImageToPyTorch(env)\n",
    "env = FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = t.cuda.is_available()# and False\n",
    "device = t.device('cuda') if USE_CUDA else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, env, model, eps, eps_final, eps_steps, initial_explore=0):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_decay = np.exp(np.log(eps_final / eps) / eps_steps)\n",
    "        self.initial_explore = initial_explore\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.initial_explore > 0:\n",
    "            self.initial_explore -= 1\n",
    "            return self.env.action_space.sample()\n",
    "        self.eps = max(self.eps_final, self.eps * self.eps_decay)\n",
    "        if random.random() < self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        self.model.eval()\n",
    "        state = t.FloatTensor(np.array(state)).to(device)\n",
    "        q = self.model(state)\n",
    "        return q.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(t.nn.Module):\n",
    "    def __init__(self, input_shape, input_frames, n_out):\n",
    "        super().__init__()\n",
    "        self.cnn = t.nn.Sequential(\n",
    "            t.nn.Conv2d(3 * input_frames, 32, kernel_size=8, stride=4),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            t.nn.PReLU(),\n",
    "        ) # -> 64 9 6\n",
    "        cnn_fc = self.feature_size(self.cnn, input_shape)\n",
    "        self.fc = t.nn.Sequential(\n",
    "            t.nn.Linear(cnn_fc, 512),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Linear(512, n_out)\n",
    "        )\n",
    "        self.apply(self.weights_init)\n",
    "    \n",
    "    def feature_size(self, cnn, shape):\n",
    "        return cnn(t.zeros(1, *shape)).view(1, -1).size(1)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, t.nn.Linear):\n",
    "            t.nn.init.kaiming_normal_(m.weight, 2)\n",
    "            t.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, t.nn.Conv2d):\n",
    "            t.nn.init.kaiming_normal_(m.weight, 2)\n",
    "            t.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) < 4:\n",
    "            x = x.unsqueeze(0)\n",
    "        p = self.cnn(x)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        return self.fc(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# distutils: language = c++\n",
    "cimport numpy as np\n",
    "import numpy as np\n",
    "\n",
    "cdef class SumTree(object):\n",
    "    cdef size_t capacity\n",
    "    cdef np.ndarray tree\n",
    "    cdef size_t data_pointer\n",
    "    cdef np.ndarray data\n",
    "\n",
    "    def __init__(self, capacity : int):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(capacity * 2 - 1, dtype=np.float32)\n",
    "        self.data_pointer = 0\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    def add(self, p : float, data : object):\n",
    "        cdef size_t tree_idx\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_idx, p)\n",
    "        \n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:\n",
    "            self.data_pointer = 0\n",
    "    \n",
    "    def update(self, tree_idx : size_t, p : float):\n",
    "        cdef float change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_total_p(self) -> float:\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def get_leaf(self, v : float):\n",
    "        cdef size_t parent_idx = 0\n",
    "        cdef size_t cl_idx\n",
    "        cdef size_t cr_idx\n",
    "        while True:\n",
    "            cl_idx = 2 * parent_idx + 1\n",
    "            cr_idx = cl_idx + 1\n",
    "            if  cl_idx >= self.capacity * 2 - 1:\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if v <= self.tree[cl_idx] or self.tree[cr_idx] == 0.0:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "    \n",
    "    def get_max_p(self) -> float:\n",
    "        return np.max(self.tree[-self.capacity:])\n",
    "    \n",
    "    def get_min_p(self) -> float:\n",
    "        return np.min(self.tree[-self.capacity:])\n",
    "    \n",
    "    def get_capacity(self) -> size_t:\n",
    "        return self.capacity\n",
    "\n",
    "class PriorExpReplay(object):\n",
    "    def __init__(self, capacity : size_t):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.beta = 0.4\n",
    "\n",
    "    def store(self, transition : object):\n",
    "        cdef float max_p = self.tree.get_max_p()\n",
    "        if max_p == 0.0:\n",
    "            max_p = 1.0\n",
    "        self.tree.add(max_p, transition)\n",
    "        \n",
    "    def sample(self, n : size_t):\n",
    "        cdef int [:] b_idx = np.empty((n,), dtype=np.int32)\n",
    "        cdef object[:] b_memory = np.empty((n,), dtype=object)\n",
    "        cdef float[:] ISWeights = np.empty((n,), dtype=np.float32)\n",
    "        cdef float pri_seg = self.tree.get_total_p() / float(n)\n",
    "        cdef float min_prob, prob\n",
    "        self.beta = np.min([1., self.beta + 0.001])\n",
    "        \n",
    "        min_prob = self.tree.get_min_p() / self.tree.get_total_p()\n",
    "           \n",
    "        cdef float a, b, v, p\n",
    "        cdef object data\n",
    "        cdef size_t i, idx\n",
    "        for i in range(n):\n",
    "            a = pri_seg * i\n",
    "            b = pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.get_total_p()\n",
    "            ISWeights[i] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "    \n",
    "    def batch_update(self, tree_idx : np.ndarray, abs_errors : np.ndarray):\n",
    "        cdef np.ndarray clipped_errors = np.clip(abs_errors, 0.01, 1.0)\n",
    "        cdef np.ndarray ps = np.power(clipped_errors, 0.6)\n",
    "        cdef size_t i\n",
    "        for i in range(len(ps)):\n",
    "            self.tree.update(tree_idx[i], ps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = PriorExpReplay(maxlen)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.memory.store((state, action, next_state, reward, done))\n",
    "    def sample(self, n):\n",
    "        with t.no_grad():\n",
    "            indices, memory, weights = self.memory.sample(n)\n",
    "            states, actions, next_states, rewards, masks = zip(*memory)\n",
    "            actions = t.LongTensor(actions).to(device)\n",
    "            rewards = t.FloatTensor(rewards).to(device)\n",
    "            masks = 1 - t.FloatTensor(masks).to(device)\n",
    "            states = Replay.stack_states(states)\n",
    "            next_states = Replay.stack_states(next_states)\n",
    "            return states, actions, next_states, rewards, masks\n",
    "    def batch_update(self, indices, abs_errors):\n",
    "        self.memory.batch_update(indices, abs_errors)\n",
    "    @staticmethod\n",
    "    def stack_states(states):\n",
    "        s = np.concatenate([np.expand_dims(x, 0) for x in states])\n",
    "        return t.ByteTensor(s).to(device).float()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "def build_model():\n",
    "    return Model(env.observation_space.shape, 4, env.action_space.n).to(device)\n",
    "\n",
    "def reset_env(env):\n",
    "    state = env.reset()\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        state, _, _, _ = env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "    return state\n",
    "\n",
    "def plot_state(msg):\n",
    "    clear_output(False)\n",
    "    plot.figure(figsize=(24,5))\n",
    "    plot.suptitle(f'{msg}, mean last 100 reward = {mean_reward}')\n",
    "    plot.subplot(131)\n",
    "    plot.title('rewards (frame=%dk, %d episodes)' % (np.round(frame/1000), episode))\n",
    "    plot.plot(all_rewards[:-1])\n",
    "    plot.subplot(132)\n",
    "    plot.title('losses')\n",
    "    plot.plot(losses)\n",
    "    plot.subplot(133)\n",
    "    plot.title('random screen')\n",
    "    state, _, _, _, _ = replay.sample(1)\n",
    "    plot.imshow(state.squeeze(0)[-3:].permute(1, 2, 0).cpu().numpy() / 255)\n",
    "    plot.show();\n",
    "\n",
    "def learn_on_replay():\n",
    "    model.train()\n",
    "\n",
    "    states, actions, next_states, rewards, masks = replay.sample(batch_size)\n",
    "    \n",
    "    q_values = model(states)\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    q_next_value = model(next_states).max(1).values.detach()\n",
    "\n",
    "    target = rewards + gamma * masks * q_next_value\n",
    "    if False:\n",
    "        loss = (target.detach() - q_value).pow(2).mean()\n",
    "    else:\n",
    "        loss = loss_fn(q_value, target.detach())\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "model = build_model()\n",
    "target_model = model\n",
    "opt =  t.optim.RMSprop(model.parameters(), lr=1e-6)\n",
    "loss_fn = t.nn.MSELoss()\n",
    "\n",
    "replay = Replay(100000)\n",
    "actor = Actor(env, model, eps=1, eps_final=0.1, eps_steps=50000, initial_explore=50000)\n",
    "all_rewards = []\n",
    "losses = collections.deque(maxlen=10000)\n",
    "batch_size = 16\n",
    "gamma = 0.99\n",
    "frame = 0\n",
    "mean_reward = 0\n",
    "\n",
    "for episode in range(10000):\n",
    "    all_rewards.append(0)\n",
    "    state, done = reset_env(env), False\n",
    "    while not done:\n",
    "        frame += 1\n",
    "        action = actor.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        all_rewards[-1] += reward\n",
    "        replay.add(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay) > batch_size:\n",
    "            learn_on_replay()\n",
    "        \n",
    "        if frame % 1000 == 0:\n",
    "            plot_state('in progress')\n",
    "\n",
    "    mean_reward = np.mean(all_rewards[-100:])\n",
    "    solved = len(all_rewards) > 100 and mean_reward > 18\n",
    "    if solved:\n",
    "        break\n",
    "\n",
    "plot_state('solved' if solved else 'finished')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if solved:\n",
    "    import gzip\n",
    "    with gzip.open('model-pong-raw-dqn.gz', 'wb') as f:\n",
    "        t.save(model.state_dict(), f, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gzip\n",
    "with gzip.open('model-pong-raw-dqn.gz', 'rb') as f:\n",
    "    model = Model().to(device)\n",
    "    model.load_state_dict(t.load(f))\n",
    "\n",
    "env = gym.make('Pong' + SUFFIX)\n",
    "env = gym.wrappers.Monitor(env, '.', force=True)\n",
    "env = ResizeObservation(env)\n",
    "env = ImageToPyTorch(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    state = t.FloatTensor(np.array(state)).to(device)\n",
    "    action = model(state).argmax().item()\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    state = t.FloatTensor(np.array(state)).to(device)\n",
    "    action = model(state).argmax().item()\n",
    "    if random.random() < 0.03:\n",
    "        action = env.action_space.sample()\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "env.close()\n",
    "!ls /mnt/sdb/openai/pong"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
