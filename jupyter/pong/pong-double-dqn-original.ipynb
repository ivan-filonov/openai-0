{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### based on https://github.com/higgsfield/RL-Adventure and https://medium.com/swlh/introduction-to-reinforcement-learning-coding-sarsa-part-4-2d64d6e37617\n",
    "\n",
    "[Double DQN @arxiv](https://arxiv.org/pdf/1509.06461.pdf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (48 << 30, hard))\n",
    "#resource.getrusage(resource.RUSAGE_SELF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch as t\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%cython -c=-O2\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cdef class cython_bufffer:\n",
    "    cdef np.ndarray np_data\n",
    "    cdef object[:] data\n",
    "    cdef size_t maxlen\n",
    "    cdef size_t write\n",
    "    cdef size_t count\n",
    "    \n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.write = 0\n",
    "        self.count = 0\n",
    "        self.np_data = np.empty((maxlen,), dtype=object)\n",
    "        self.data = self.np_data\n",
    "    \n",
    "    cpdef size_t get_count(self):\n",
    "        return self.count\n",
    "    \n",
    "    cpdef add(self, exp : object):\n",
    "        self.data[self.write] = exp\n",
    "        self.write = (self.write + 1) % self.maxlen\n",
    "        self.count = min(self.maxlen, self.count + 1)\n",
    "    \n",
    "    cpdef sample(self, n : size_t):\n",
    "        cdef np.ndarray indices = np.random.choice(self.count, n).astype(np.int32)\n",
    "        cdef np.ndarray samples = np.empty((n,), dtype=object)\n",
    "        cdef size_t i\n",
    "        for i in range(n):\n",
    "            samples[i] = self.data[indices[i]]\n",
    "        return indices, samples\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = cython_bufffer(maxlen)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.buffer.get_count()\n",
    "\n",
    "    def add(self, exp):\n",
    "        self.buffer.add(exp)\n",
    "\n",
    "    def sample(self, n):\n",
    "        return self.buffer.sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not belive how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = np.concatenate(self._frames, axis=0)\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Change image shape to CWH\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n",
    "    \n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = collections.deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ResizeObservation, self).__init__(env)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0] // 2, shp[1] // 2, shp[2]))\n",
    "        self.resize_to = (shp[1] // 2, shp[0] // 2)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return cv2.resize(observation, self.resize_to, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "SUFFIX = 'NoFrameskip-v4'\n",
    "env = gym.make('Pong' + SUFFIX)\n",
    "env = ResizeObservation(env)\n",
    "env = ImageToPyTorch(env)\n",
    "env = FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = t.cuda.is_available()# and False\n",
    "device = t.device('cuda') if USE_CUDA else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, env, model, eps, eps_final, eps_steps, initial_explore=0):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_decay = np.exp(np.log(eps_final / eps) / eps_steps)\n",
    "        self.initial_explore = initial_explore\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.initial_explore > 0:\n",
    "            self.initial_explore -= 1\n",
    "            return self.env.action_space.sample()\n",
    "        self.eps = max(self.eps_final, self.eps * self.eps_decay)\n",
    "        if random.random() < self.eps:\n",
    "            return self.env.action_space.sample()\n",
    "        self.model.eval()\n",
    "        state = t.FloatTensor(np.array(state)).to(device)\n",
    "        q = self.model(state)\n",
    "        return q.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(t.nn.Module):\n",
    "    def __init__(self, input_shape, input_frames, n_out):\n",
    "        super().__init__()\n",
    "        self.cnn = t.nn.Sequential(\n",
    "            t.nn.Conv2d(3 * input_frames, 32, kernel_size=8, stride=4),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            t.nn.PReLU(),\n",
    "        ) # -> 64 9 6\n",
    "        cnn_fc = self.feature_size(self.cnn, input_shape)\n",
    "        self.fc = t.nn.Sequential(\n",
    "            t.nn.Linear(cnn_fc, 512),\n",
    "            t.nn.PReLU(),\n",
    "            t.nn.Linear(512, n_out)\n",
    "        )\n",
    "        self.apply(self.weights_init)\n",
    "    \n",
    "    def feature_size(self, cnn, shape):\n",
    "        return cnn(t.zeros(1, *shape)).view(1, -1).size(1)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, t.nn.Linear):\n",
    "            t.nn.init.kaiming_normal_(m.weight, 2)\n",
    "            t.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, t.nn.Conv2d):\n",
    "            t.nn.init.kaiming_normal_(m.weight, 2)\n",
    "            t.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) < 4:\n",
    "            x = x.unsqueeze(0)\n",
    "        p = self.cnn(x)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        return self.fc(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = np.empty(maxlen, dtype=np.object)\n",
    "        self.index = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.memory[self.index] = (state, action, next_state, reward, done)\n",
    "        self.index = (self.index + 1) % len(self.memory)\n",
    "        if self.count < len(self.memory):\n",
    "            self.count += 1\n",
    "\n",
    "    def sample(self, n, device):\n",
    "        with t.no_grad():\n",
    "            indices = np.random.randint(low=0, high=self.count, size=n)\n",
    "            samples = zip(*list(self.memory[indices]))\n",
    "            states, actions, next_states, rewards, masks = samples\n",
    "\n",
    "            actions = t.LongTensor(actions).to(device)\n",
    "            rewards = t.FloatTensor(rewards).to(device)\n",
    "            masks = t.FloatTensor(masks).to(device)\n",
    "            states = Replay.stack_states(states, device)\n",
    "            next_states = Replay.stack_states(next_states, device)\n",
    "            return states, actions, next_states, rewards, masks\n",
    "\n",
    "    @staticmethod\n",
    "    def stack_states(states, device):\n",
    "        s = np.concatenate([np.expand_dims(x, 0) for x in states])\n",
    "        return t.ByteTensor(s).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return Model(env.observation_space.shape, 4, env.action_space.n).to(device)\n",
    "\n",
    "def reset_env(env):\n",
    "    state = env.reset()\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        state, _, _, _ = env.step(env.unwrapped.get_action_meanings().index('FIRE'))\n",
    "    return state\n",
    "\n",
    "def plot_state(msg):\n",
    "    clear_output(False)\n",
    "    plot.figure(figsize=(18,5))\n",
    "    mean_reward = 0 if len(mean_rewards) < 2 else mean_rewards[-2]\n",
    "    plot.suptitle(f'DQN, double, original, {msg}, mean last 100 reward = {mean_reward}')\n",
    "    plot.subplot(131)\n",
    "    plot.title('rewards (frame=%dk, %d episodes)' % (np.round(frame/1000), episode))\n",
    "    plot.plot(np.array(all_rewards)[:-1], label='rewards')\n",
    "    plot.plot(np.array(mean_rewards)[:-1], label='mean')\n",
    "    plot.legend()\n",
    "    plot.subplot(132)\n",
    "    plot.title('losses')\n",
    "    plot.plot(losses)\n",
    "    plot.subplot(133)\n",
    "    plot.title('random screen')\n",
    "    state, _, _, _, _ = replay.sample(1, device)\n",
    "    plot.imshow(state.squeeze(0)[-3:].permute(1, 2, 0).cpu().numpy() / 255)\n",
    "    plot.savefig('state-pong-double-dqn-original.png', format='png')\n",
    "    plot.show()\n",
    "    plot.close()\n",
    "\n",
    "def learn_on_replay():\n",
    "    target_model.eval()\n",
    "    model.train()\n",
    "\n",
    "    states, actions, next_states, rewards, masks = replay.sample(batch_size, device)\n",
    "    \n",
    "    q_values = model(states)\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    with t.no_grad():\n",
    "        q_next_values = target_model(next_states)\n",
    "        if ORIGINAL_DDQN:\n",
    "            q_next_action = model(next_states).max(1).indices\n",
    "            q_next_value = q_next_values.gather(1, q_next_action.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            q_next_value = q_next_values.max(1).values\n",
    "        target = rewards + gamma * (1 - masks) * q_next_value\n",
    "\n",
    "    loss = loss_fn(q_value, target.detach())\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "ORIGINAL_DDQN = True\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "opt =  t.optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = t.nn.SmoothL1Loss()\n",
    "\n",
    "replay = Replay(int(1e6))\n",
    "actor = Actor(env, model, eps=1, eps_final=0.05, eps_steps=100000, initial_explore=100000)\n",
    "all_rewards = collections.deque(maxlen=1000)\n",
    "mean_rewards = collections.deque(maxlen=1000)\n",
    "losses = collections.deque(maxlen=10000)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "frame = 0\n",
    "\n",
    "for episode in range(10000):\n",
    "    all_rewards.append(0)\n",
    "    mean_rewards.append(0)\n",
    "    state, done = reset_env(env), False\n",
    "    while not done:\n",
    "        frame += 1\n",
    "        action = actor.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        all_rewards[-1] += reward\n",
    "        replay.add(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay) > batch_size:\n",
    "            learn_on_replay()\n",
    "        \n",
    "        if frame % 1000 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        \n",
    "        if frame % 10000 == 0:\n",
    "            plot_state('in progress')\n",
    "\n",
    "    mean_rewards[-1] = np.mean(np.array(all_rewards)[-100:])\n",
    "    solved = len(all_rewards) > 100 and mean_rewards[-2] > 18\n",
    "    if solved:\n",
    "        break\n",
    "\n",
    "plot_state('solved' if solved else 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if solved:\n",
    "    import gzip\n",
    "    with gzip.open('model-pong-double-dqn-orig.gz', 'wb') as f:\n",
    "        t.save(model.state_dict(), f, pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gzip\n",
    "with gzip.open('model-pong-raw-dqn.gz', 'rb') as f:\n",
    "    model = Model().to(device)\n",
    "    model.load_state_dict(t.load(f))\n",
    "\n",
    "env = gym.make('Pong' + SUFFIX)\n",
    "env = gym.wrappers.Monitor(env, '.', force=True)\n",
    "env = ResizeObservation(env)\n",
    "env = ImageToPyTorch(env)\n",
    "env = FrameStack(env, 4)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    state = t.FloatTensor(np.array(state)).to(device)\n",
    "    action = model(state).argmax().item()\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    state = t.FloatTensor(np.array(state)).to(device)\n",
    "    action = model(state).argmax().item()\n",
    "    if random.random() < 0.03:\n",
    "        action = env.action_space.sample()\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "env.close()\n",
    "!ls /mnt/sdb/openai/pong"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
